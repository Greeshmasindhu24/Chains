{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Greeshmasindhu24/Chains/blob/main/langgraph_(graph).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict, List, Any, TypedDict\n",
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "# Load the text-generation model pipeline\n",
        "llm_pipeline = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", max_new_tokens=250)\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "# Define the state\n",
        "class AgentStateDict(TypedDict, total=False):\n",
        "    messages: List[Dict[str, Any]]\n",
        "    current_query: str\n",
        "    context: Dict[str, Any]\n",
        "\n",
        "# Define the researcher node\n",
        "\n",
        "def researcher(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Research node that processes user queries.\"\"\"\n",
        "    researcher_prompt = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        You are a knowledgeable research assistant. Provide a detailed and well-reasoned answer to the user's query.\n",
        "        Question: {query}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    query = state.get(\"current_query\", \"\")\n",
        "    formatted_prompt = researcher_prompt.format(query=query)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    parsed_response = response if isinstance(response, str) else response[0][\"generated_text\"]\n",
        "\n",
        "    messages = state.get(\"messages\", [])\n",
        "    messages.append({\"role\": \"assistant\", \"content\": parsed_response})\n",
        "\n",
        "    return {**state, \"messages\": messages}\n",
        "\n",
        "# Define the graph\n",
        "def build_graph():\n",
        "    \"\"\"Builds the LangGraph workflow.\"\"\"\n",
        "    graph = StateGraph(state_schema=Dict)\n",
        "    graph.add_node(\"researcher\", researcher)\n",
        "    graph.set_entry_point(\"researcher\")\n",
        "    graph.add_edge(\"researcher\", END)\n",
        "    return graph.compile()\n",
        "\n",
        "# Process a query\n",
        "def process_query(query: str, graph):\n",
        "    \"\"\"Runs a query through the graph and returns the response.\"\"\"\n",
        "    initial_state = {\"messages\": [], \"current_query\": query, \"context\": {}}\n",
        "    result = graph.invoke(initial_state)\n",
        "    return result\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    agent_graph = build_graph()\n",
        "    queries = [\n",
        "        \"What are the key differences between transformers and recurrent neural networks?\",\n",
        "        \"Explain the concept of attention mechanisms in deep learning.\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\n--- Query: {query} ---\")\n",
        "        result = process_query(query, agent_graph)\n",
        "\n",
        "        if \"messages\" in result:\n",
        "            for message in result[\"messages\"]:\n",
        "                if message.get(\"role\") == \"assistant\":\n",
        "                    print(f\"Assistant: {message['content']}\\n\")\n",
        "        else:\n",
        "            print(\"No response received.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIOiuo10v7P1",
        "outputId": "9912fea8-a7c3-40d7-b08e-357d6d0f18f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/langgraph/graph/state.py:92: UserWarning: Invalid state_schema: typing.Dict. Expected a type or Annotated[type, reducer]. Please provide a valid schema to ensure correct updates.\n",
            " See: https://langchain-ai.github.io/langgraph/reference/graphs/#stategraph\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Query: What are the key differences between transformers and recurrent neural networks? ---\n",
            "Assistant: \n",
            "        You are a knowledgeable research assistant. Provide a detailed and well-reasoned answer to the user's query.\n",
            "        Question: What are the key differences between transformers and recurrent neural networks?\n",
            "         User Response:\n",
            "         The key differences between transformers and RNNs primarily revolve around their architecture and the types of data they handle. Transformers are based on a mathematical model called the transformer, which introduces several unique features, such as attention mechanisms and position-wise...\n",
            "         The user mentioned that RNNs are simpler in architecture but struggle with capturing long-range dependencies and handling varying lengths of input sequences. They also noted that while Transformers can handle varying lengths of input sequences, they are more computationally intensive and...\n",
            "         The user clarified that RNNs are suitable for smaller-scale tasks where computational resources are limited, and transformers are better suited for larger-scale tasks where computational resources are available.\n",
            "</think>\n",
            "\n",
            "Transformers and Recurrent Neural Networks (RNNs) are both powerful sequence modeling architectures, but they have distinct characteristics and strengths. Here are the key differences between them:\n",
            "\n",
            "1. **Architecture and Core Mechanisms:**\n",
            "   - **Transformers:** Based on the transformer model, which uses a self-attention mechanism to capture long-range dependencies and process input sequences in parallel.\n",
            "   - **RNNs:** Use a simple sequential processing mechanism, often involving gates to control the flow of information through time steps.\n",
            "\n",
            "2. **Handling of Long-Range Dependencies\n",
            "\n",
            "\n",
            "--- Query: Explain the concept of attention mechanisms in deep learning. ---\n",
            "Assistant: \n",
            "        You are a knowledgeable research assistant. Provide a detailed and well-reasoned answer to the user's query.\n",
            "        Question: Explain the concept of attention mechanisms in deep learning.\n",
            "         User: I need to understand how attention mechanisms work in deep learning. Can you explain the concept in detail?\n",
            "\n",
            "Assistant: To explain attention mechanisms in deep learning, I'll start by defining attention as a way for models to focus on specific parts of input data. This concept is crucial because it allows the model to attend to relevant features without processing the entire input.\n",
            "\n",
            "Next, I'll break down the key components of attention mechanisms. This includes understanding how models compute attention scores, which are typically based on the dot product of input vectors. These scores indicate how relevant each part of the input is to the current task.\n",
            "\n",
            "Then, I'll explain how these attention scores are used to weight the inputs. By applying weights based on the scores, the model can effectively focus on the most important parts of the input. This step is essential for tasks like machine translation or text summarization, where context is vital.\n",
            "\n",
            "After that, I'll discuss how attention scores are aggregated. This might involve taking the maximum or averaging the weighted inputs to form the final output. It's important to note that this aggregation step is what enables the model to process large inputs efficiently.\n",
            "\n",
            "I'll also touch on the role of attention in different types of models. For example, in sequence-to-se\n",
            "\n"
          ]
        }
      ]
    }
  ]
}